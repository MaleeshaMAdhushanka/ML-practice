{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a79b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Imports ---\n",
    "import wikipediaapi\n",
    "import nltk\n",
    "import ssl\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# Now, try to download the data again\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "print(\"NLTK data downloaded successfully using the manual method.\")\n",
    "\n",
    "print(\"Libraries imported and NLTK data downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8743a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Fetch Wikipedia Articles ---\n",
    "\n",
    "# List of articles to cluster. We've chosen topics in astronomy, biology, and computer science.\n",
    "article_titles = [\n",
    "    \"Galaxy\", \"Black hole\", \"Supernova\", # Astronomy\n",
    "    \"DNA\", \"Photosynthesis\", \"Evolution\", # Biology\n",
    "    \"Machine learning\", \"Artificial intelligence\", \"Computer programming\" # Computer Science\n",
    "]\n",
    "\n",
    "# Initialize the Wikipedia API\n",
    "wiki_api = wikipediaapi.Wikipedia('MyClusteringProject/1.0', 'en')\n",
    "\n",
    "documents = []\n",
    "for title in article_titles:\n",
    "    page = wiki_api.page(title)\n",
    "    if page.exists():\n",
    "        documents.append(page.text)\n",
    "        print(f\"Successfully fetched: {title}\")\n",
    "    else:\n",
    "        print(f\"Could not find page: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99908c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Preprocess the Text ---\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    words = text.split()\n",
    "    # Remove stop words and lemmatize\n",
    "    processed_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "processed_documents = [preprocess_text(doc) for doc in documents]\n",
    "print(\"Text preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceea48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Convert Text to Vectors ---\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000) # Limit to the top 1000 features\n",
    "\n",
    "# Create the TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_documents)\n",
    "\n",
    "print(tfidf_matrix)\n",
    "\n",
    "print(\"TF-IDF matrix created successfully.\")\n",
    "print(f\"Shape of the matrix: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0893da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Run K-Means ---\n",
    "\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "## The below line is functionally identical to the above line\n",
    "#kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42, n_init=10)\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "# Get the cluster assignments for each document\n",
    "labels = kmeans.labels_\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a1585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6: Analyze the Results ---\n",
    "\n",
    "# Group document titles by cluster\n",
    "clusters = {i: [] for i in range(k)}\n",
    "for i, label in enumerate(labels):\n",
    "    clusters[label].append(article_titles[i])\n",
    "\n",
    "# Get the top terms per cluster\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(k):\n",
    "    print(f\"--- Cluster {i} ---\")\n",
    "    print(f\"Documents: {clusters[i]}\")\n",
    "    \n",
    "    top_terms = [terms[ind] for ind in order_centroids[i, :10]]\n",
    "    print(f\"Top Keywords: {top_terms}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e106c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 7: Putting the Model to Work - Predicting on New Documents ---\n",
    "# Now for the exciting part! We can take our final \"trained\" model and \n",
    "# use it to instantly categorize a brand new, unseen document. \n",
    "# Let's see which topic cluster it belongs to!\n",
    "\n",
    "# --- Define your new document ---\n",
    "new_text = \"An algorithm is a set of well-defined instructions designed to perform a specific task or solve a computational problem. In computer science, the study of algorithms is fundamental to creating efficient and scalable software. Data structures, such as arrays and hash tables, are used to organize data in a way that allows these algorithms to access and manipulate it effectively.\"\n",
    "\n",
    "# --- Apply the SAME preprocessing ---\n",
    "# We use the preprocess_text function we defined earlier\n",
    "processed_new_text = preprocess_text(new_text)\n",
    "print(f\"Cleaned Text: {processed_new_text}\")\n",
    "\n",
    "# --- Use the FITTED vectorizer to transform the text ---\n",
    "# IMPORTANT: Use .transform(), not .fit_transform()\n",
    "# This ensures it uses the same vocabulary learned from the original documents.\n",
    "new_tfidf_vector = vectorizer.transform([processed_new_text])\n",
    "\n",
    "print(f\"\\nShape of the new vector: {new_tfidf_vector.shape}\")\n",
    "\n",
    "# --- Now you can predict its cluster ---\n",
    "predicted_label = kmeans.predict(new_tfidf_vector)\n",
    "\n",
    "print(f\"\\nThe new document belongs to cluster: {predicted_label[0]}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
